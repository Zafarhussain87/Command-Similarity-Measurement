{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re \n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kernel.org/doc/man-pages/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "url_man_1_8 =['https://man7.org/linux/man-pages/dir_section_1.html', 'https://man7.org/linux/man-pages/dir_section_8.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pars(pre):\n",
    "    pars_text = pre.text.split('\\n')\n",
    "    pars_indices = []\n",
    "    for p in range(len(pars_text)):\n",
    "        if len(pars_text[p])-len(pars_text[p].lstrip())==7 and (pars_text[p].split(' ')[7].startswith('−') or \n",
    "                                                            pars_text[p].split(' ')[7].startswith('-')):\n",
    "            pars_indices.append(p)\n",
    "        elif len(pars_text[p])-len(pars_text[p].lstrip())==6 and (pars_text[p].split(' ')[6].startswith('−') or \n",
    "                                                            pars_text[p].split(' ')[6].startswith('-')):\n",
    "            pars_indices.append(p)\n",
    "        elif len(pars_text[p])-len(pars_text[p].lstrip())==5 and (pars_text[p].split(' ')[5].startswith('−') or \n",
    "                                                            pars_text[p].split(' ')[5].startswith('-')):\n",
    "            pars_indices.append(p)\n",
    "    extra_pars = ['-help', '--help','-help','−−help' '-version','-?','--?', '--version','−version', '−−version', '-V, --version',\n",
    "                 '-?, --help', '−?, −−help','-V,--version', '-?,--help','−?,−−help', '-H,--HELP', '--HELP',  '−−VERSION',\n",
    "                   '−VERSION', '−−VERSION','-?,--HELP', '−?,−−HELP' , '-h,--HELP']\n",
    "    #extra_pars = []\n",
    "    pars = dict()\n",
    "    if len(pars_indices)>=1:\n",
    "        for p in range(1,len(pars_indices)):\n",
    "            start_ind = pars_indices[p-1]\n",
    "            end_ind = pars_indices[p]\n",
    "            parm = pars_text[start_ind].split()\n",
    "\n",
    "            if parm[0] not in extra_pars and len(parm)>3:\n",
    "                parm_key = parm[0]\n",
    "                parm_desc = parm[1:]\n",
    "                parm_desc += pars_text[start_ind+1:end_ind]\n",
    "                parm_desc = ' '.join(parm_desc)\n",
    "                pars[parm_key]=parm_desc\n",
    "            elif parm[0] not in extra_pars:\n",
    "                parm_key = ' '.join(parm)\n",
    "                parm_desc = pars_text[start_ind+1:end_ind]\n",
    "                parm_desc = ''.join(parm_desc)\n",
    "                pars[parm_key]=parm_desc\n",
    "        last_ind = pars_indices[-1]\n",
    "        tot = len(pars_text)\n",
    "        last_ind_key = pars_text[last_ind].split()\n",
    "        if last_ind_key[0] not in extra_pars and len(last_ind_key)>4:\n",
    "            parm_key = last_ind_key[0]\n",
    "            parm_desc = last_ind_key[1:]\n",
    "            parm_desc = ' '.join(parm_desc)\n",
    "            for r in range(1,tot-last_ind):\n",
    "                if len(pars_text[last_ind+r])>0:\n",
    "                    parm_desc+=pars_text[last_ind+r]\n",
    "                else:\n",
    "                    break\n",
    "            pars[parm_key] = parm_desc\n",
    "        elif last_ind_key[0] not in extra_pars:\n",
    "            parm_key = last_ind_key[0]\n",
    "            parm_desc = ''  \n",
    "            for r in range(1,tot-last_ind):\n",
    "                if len(pars_text[last_ind+r])>0:\n",
    "                    parm_desc+=pars_text[last_ind+r]\n",
    "                else:\n",
    "                    break\n",
    "            pars[parm_key] = parm_desc\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    else:\n",
    "        pars['None']='None'\n",
    "    \n",
    "    return pars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synopsis(pre):\n",
    "    syn = pre.text.split('\\n')  \n",
    "    syn_pars = []\n",
    "    for si in range(len(syn)):\n",
    "        if len(syn[si])>1:\n",
    "            syn_pars.append(syn[si].strip())\n",
    "    return syn_pars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={}\n",
    "c=0\n",
    "data['commands'] = []\n",
    "for new_url in url_man_1_8:\n",
    "    #new_url =l.a.get('href')\n",
    "    response = requests.get(new_url)\n",
    "    soup = BeautifulSoup(response.content.decode('utf-8','ignore'), 'html.parser')\n",
    "    base_url = 'https://man7.org/linux/man-pages/'\n",
    "    for h1 in soup.find_all('a'):\n",
    "        if h1.get('href').startswith('./man'):\n",
    "            new_urls = base_url+h1.get('href')[2:]\n",
    "            response = requests.get(new_urls)\n",
    "            soup = BeautifulSoup(response.content.decode('utf-8','ignore'), 'html.parser')\n",
    "            comd_name = soup.title.text.split('(')[0]\n",
    "            comd_name_one = soup.title.text.split('(')[1]\n",
    "\n",
    "            parameters = dict()\n",
    "            headers_names = []\n",
    "            for h2 in soup.findAll('h2'):\n",
    "                headers_names.append(h2.a.get('href'))\n",
    "\n",
    "            \n",
    "            if ('#SYNOPSIS' in headers_names or '#Synopsis' in headers_names) and (comd_name_one.startswith('1)') or comd_name_one.startswith('8)')):\n",
    "                print(comd_name)\n",
    "                for h2 in soup.findAll('h2'):\n",
    "                    if h2.a.get('href')=='#NAME' or h2.a.get('href')=='#Name':\n",
    "                        if h2.next_sibling.name=='pre':\n",
    "                            tn = h2.next_sibling\n",
    "                            comd_desc=tn.text\n",
    "                        else:\n",
    "                            pass\n",
    "                    elif h2.a.get('href')=='#DESCRIPTION' or h2.a.get('href')=='#Description':\n",
    "                        if h2.next_sibling.name=='pre':\n",
    "                            tt = h2.next_sibling\n",
    "                            comd_desc+=tt.get_text()\n",
    "                        else:\n",
    "                            pass\n",
    "                    elif h2.a.get('href')=='#OPTIONS' or h2.a.get('href')=='#Options':\n",
    "                        #parameters = extract_parameters_desc(h2)\n",
    "                        if h2.next_sibling.name=='pre':\n",
    "                            pre_p = h2.next_sibling\n",
    "                            pp = pre_p.text.split('\\n')\n",
    "                            len_pp = len(pp)\n",
    "                            if len_pp<=3 and ( 'None' in pp[1] or len(pp[1]) ==0):\n",
    "                                parameters['None']='None'\n",
    "                       \n",
    "                            else: \n",
    "                                parameters = extract_pars(pre_p)\n",
    "                                \n",
    "                        else:\n",
    "                            pass\n",
    "                \n",
    "                    elif (h2.a.get('href')=='#SYNOPSIS' or h2.a.get('href')=='#Synopsis') and h2.next_sibling.name=='pre':\n",
    "                            pre_s = h2.next_sibling\n",
    "                            synopsis_data = extract_synopsis(pre_s)\n",
    "        \n",
    "                \n",
    "                if len(parameters)<1:\n",
    "                    c+=1\n",
    "                    parameters = extract_pars(tt)\n",
    "                    comd_desc = ''\n",
    "                    comd_desc+=tt.get_text()\n",
    "                    comd_desc = re.split(r'\\n\\n\\s+-',comd_desc)[0]\n",
    "                            \n",
    "                comd_desc = comd_desc.replace('\\n', '')\n",
    "                comd_desc = re.sub(' +', ' ', comd_desc)\n",
    "                data['commands'].append({\n",
    "                 'commands': comd_name,\n",
    "                'description':comd_desc,\n",
    "                 'synopsis':synopsis_data,\n",
    "                'parameters':parameters})\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('linux_data_raw.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
